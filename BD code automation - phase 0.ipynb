{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This code performs \"Phase 0\" of the PFFP data processing automation. It performs the following tasks:\n",
    "\n",
    "* reads all files in a folder of BD deployments\n",
    "* identifies if a BD file contains a drop or not\n",
    "* adds all files containing drops to a separate \"Drops Only\" folder\n",
    "* creates a table of the files with drops, the time of file creation, and the number of drops in the file\n",
    "* exports the above table to the \"Drops Only\" folder\n",
    "\n",
    "The required user inputs are as follows:\n",
    "\n",
    "* file path to folder containing all .bin files\n",
    "* file path to the \"Drops Only\" folder\n",
    "* bluedrop used and associated calibration factors (maybe find a way out of this?)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP VARIABLES - USER INPUTS\n",
    "BD = 3 #Bluedrop file is from \n",
    "\n",
    "# paste the filepath to the folder where the BD data is stored\n",
    "binFolderPath = 'C:/Users/elise/Desktop/BD3 3.16.23 - All Files'\n",
    "\n",
    "#paste the filepath to an excel file that the analysis results will be printed in\n",
    "outputPath = 'C:/Users/elise/Desktop/Test data - BD3 3.16.23 -Drops Only' #  Path to pre-existing Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"blog01C5.bin\"\n",
    "\n",
    "# read file, create dataframe, calibrate, filter pore pressure\n",
    "def readfile(filename):\n",
    "    global df\n",
    "    global Time\n",
    "    global count\n",
    "\n",
    "    global g2g\n",
    "    global g18g \n",
    "    global g50g \n",
    "    global ppm \n",
    "    global g200g \n",
    "    global gX55g\n",
    "    global gY55g \n",
    "    global g250g\n",
    "    global ppm_df\n",
    "    global ppm_avg\n",
    "    \n",
    "    file_path = os.path.join(binFolderPath, filename)  # Create the full file path\n",
    "    if os.path.isfile(file_path): #for every file in the folder\n",
    "\n",
    "        file = open(file_path, 'rb')  # read file\n",
    "        element = file.read(3)  # create a byte list with each element having 3 bytes\n",
    "        data_array = []\n",
    "\n",
    "        while element:\n",
    "            iVAl = int.from_bytes(element, byteorder='big', signed=True) # Convert to signed integer before adding to data array\n",
    "            data_array.append(iVAl)  # adds the reshaped data from the bd file to the data frame\n",
    "            element = file.read(3)\n",
    "            \n",
    "        file.close()\n",
    "\n",
    "        np_array = np.array(data_array)  # create numpy array from the list\n",
    "        np_array = np.reshape(np_array, (-1, 10))  # convert the 1d array to 2d array with 10 cols\n",
    "\n",
    "    df = pd.DataFrame(np_array) # Creates a Dataframe in pandas from the bd data\n",
    "    df.columns = ['Count', 'File ID', 'g2g', 'g18g', 'g50g', 'ppm', 'g200g', 'gX55g', 'gY55g', 'g250g'] # names columns\n",
    "    #print(df)\n",
    "\n",
    "    #calibrate dataframe\n",
    "\n",
    "    # APPLY CALIBRATION FACTORS\n",
    "    Time = (df['Count']-df['Count'].iloc[0]+1)/2000 # gives time in s\n",
    "    count = df[\"Count\"]-df['Count'].iloc[0]\n",
    "\n",
    "    if BD == 3:  # calibration factors from March 2023\n",
    "        g2g = (df['g2g']-33570.1)/1614577.9 #- offset# accelerometers are in g\n",
    "        g18g = (df['g18g']+13495)/163387.2 #- offset\n",
    "        g50g = (df['g50g']-238817.4)/63779.5 #- offset\n",
    "        ppm = ((df['ppm']-139040.1)/20705) * 6.89475729 # converts to kPa\n",
    "        g200g = (df['g200g'] -262332.4)/38888.7 #- offset\n",
    "        gX55g = (df['gX55g']-70406)/59754.3\n",
    "        gY55g = (df['gY55g']-69421.1)/141871.5\n",
    "        g250g = (df['g250g']-39077.4)/13746.9 #- offset\n",
    "\n",
    "    if BD == 2: # calibration factors from March 2023\n",
    "        g2g = (df['g2g']+36597.6)/1637627.3 #- offset# accelerometers are in g\n",
    "        g18g = (df['g18g']-26185.3)/160297.2 #- offset\n",
    "        g50g = (df['g50g']-212256.9)/63968.7 #- offset\n",
    "        ppm = ((df['ppm']+55518.9)/18981.7) * 6.89475729 # converts to kPa\n",
    "        g200g = (df['g200g']-175499.4)/30583.8 #- offset\n",
    "        gX55g = (df['gX55g']-53629.9)/68590.9\n",
    "        gY55g = (df['gY55g']-43694.3)/68280.3\n",
    "        g250g = (df['g250g']-39619.9)/13545.8 #- offset\n",
    "\n",
    "    if BD == 1: # calibration factors from March 2023\n",
    "        g2g = (df['g2g']-43727.6)/1625064 #- offset # accelerometers are in g\n",
    "        g18g = (df['g18g']-45085.6)/160925.7 #- offset\n",
    "        g50g = (df['g50g']-173493.4)/63944.6 #- offset\n",
    "        ppm = ((df['ppm']+31776.1)/20679.7) * 6.89475729 # this is kPa\n",
    "        g200g = (df['g200g'] -731734.3)/32192.4  #- offset\n",
    "        gX55g = (df['gX55g'] -68837.9)/52137.3\n",
    "        gY55g = (df['gY55g']-68015.9)/28074.9\n",
    "        g250g = (df['g250g']+10580)/13688.7 #- offset\n",
    "\n",
    "    #Filter pore pressure\n",
    "    ppm_df = pd.DataFrame(ppm) #create dataframe for pore pressure only\n",
    "    ppm_df.columns = ['raw'] # recorded pore pressure in kPa\n",
    "    ppm_df['filtered'] = ppm_df['raw'].rolling(50).mean() #take rolling average over a 50-pt window\n",
    "    ppm_avg = np.average(ppm) #average ppm over whole file\n",
    "\n",
    "    ppm_df['difference'] = ppm_df['filtered'] - ppm_avg #value to compare against\n",
    "\n",
    "# Choose which accelerometer to use \n",
    "\n",
    "def accPick(): #this function picks the smallest accelerometer that's not maxed out to perform the integration on\n",
    "    maxAcc = g250g.max()\n",
    "    global acc\n",
    "    if maxAcc < 5: #- offset:\n",
    "        if g18g.max() < 1.8: #offset:  # does an extra check for the 2g because of noise\n",
    "            acc = g2g\n",
    "        else:\n",
    "            acc = g18g \n",
    "    elif maxAcc < 18: #- offset\n",
    "        acc = g18g\n",
    "    elif maxAcc < 50: #- offset:\n",
    "        acc = g50g\n",
    "    else:\n",
    "        acc = g250g\n",
    "\n",
    "#Locate the drops\n",
    "\n",
    "def locate_drops():\n",
    "    global peaksArray_trimmed\n",
    "    global nDrops_trimmed\n",
    "    global nDrops\n",
    "    global peaks\n",
    "    global x\n",
    "\n",
    "    #signal processing of deceleration data\n",
    "    x = np.array(acc)  # what accelerometer to get the peaks from\n",
    "    peaks, _ = find_peaks(x, height = 2, distance=1000, prominence=3)  # finds the largest peaks more than 2g spaced at least 10000 counts apart\n",
    "    peaksArray = np.array(peaks)  # prints a list of the count where the peaks occur\n",
    "    #peaksArray = filter(filterpeak, peaksArray)\n",
    "    #print(peaksArray)\n",
    "    q = (peaksArray.shape) #gives number of peaks\n",
    "    nDrops = int(q[0]) #number of drops in the file\n",
    "\n",
    "    #check that pore pressure is above threshold\n",
    "    difference = ppm_df[\"difference\"]\n",
    "    ppm_peaks = []\n",
    "    for peak in peaksArray:\n",
    "        peak_ppm_array = []\n",
    "        for i in range(-150, 150):\n",
    "            #print(peak+i)\n",
    "            peak_ppm_array.append(difference[peak+i])\n",
    "        #print(len(peak_ppm_array))\n",
    "        peak_ppm_max = max(peak_ppm_array)\n",
    "        peak_ppm_min = min(peak_ppm_array)\n",
    "        peak_ppm = max(peak_ppm_max, abs(peak_ppm_min)) \n",
    "        ppm_peaks.append(peak_ppm)\n",
    "        #print(peak_ppm)\n",
    "        #peak_ppm_array.append[peak_ppm]\n",
    "    #print(ppm_peaks)\n",
    "    \n",
    "    if nDrops == 0:\n",
    "        nDrops_trimmed = 0\n",
    "    elif nDrops > 0:\n",
    "        for n in range (0, nDrops):\n",
    "            #print(ppm_peaks[n])\n",
    "            if ppm_peaks[n] < 0.5:\n",
    "                peaksArray_trimmed = np.delete(peaksArray, n)\n",
    "                a = (peaksArray_trimmed.shape) #gives number of peaks\n",
    "                nDrops_trimmed = int(a[0])\n",
    "            else:\n",
    "                peaksArray_trimmed = peaksArray\n",
    "                nDrops_trimmed = nDrops\n",
    "                #print(nDrops_trimmed)\n",
    "\n",
    "#Plot deceleration, pore pressure, and identified peaks\n",
    "\n",
    "def peakplot(x, peaks): # Plot showing peak deceleration\n",
    "    #%matplotlib ipympl\n",
    "    fig, (ax1, ax2) = plt.subplots(2)\n",
    "    \n",
    "    fig.suptitle(filename)\n",
    "    ax1.plot(x, color = \"k\", label = \"Deceleration (g)\")\n",
    "    ax1.plot(peaks, x[peaks], \"x\", color = \"red\", label = \"Not a Deployment\")\n",
    "    ax1.plot(peaks, x[peaks], \"x\", color = \"dodgerblue\", label = \"Deployment\")\n",
    "    ax1.legend()\n",
    "    ax2.plot(ppm_df[\"difference\"], color = 'k', label = \"Pore Pressure difference (kPa)\")\n",
    "    #peakplot = plt.plot(ppm_df[\"filtered\"], label = \"filtered\")\n",
    "    ax2.hlines(0.5, 0, 120000, color = 'k', linestyles=\"--\")\n",
    "    ax2.hlines(-0.5, 0, 120000, color = 'k',linestyles= '--')\n",
    "    ax2.legend()\n",
    "\n",
    "    figpath = outputPath+\"/Figures\"\n",
    "    plt.savefig(figpath+\"/\"+filename.split(\".\")[0])\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Functions\n",
    "def exporttoexcel(): #MODIFY THIS TO CREATE THE DESIRED TABLE\n",
    "    with pd.ExcelWriter(outputPath, mode=\"a\", if_sheet_exists='replace') as writer:\n",
    "        output_table.to_excel(writer, sheet_name = fileNum, index=False)\n",
    "\n",
    "def troubleshooting_export():\n",
    "        with pd.ExcelWriter(troubleshootingPath, mode=\"a\", if_sheet_exists = 'new') as writer:\n",
    "            drop1.to_excel(writer, sheet_name = str(n), index = False)\n",
    "            qdyntable.to_excel(writer, sheet_name = \"qdyn\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Analysis of specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            File  Drops            Recording Time\n",
      "0   bLog01C5.bin      3  Fri Sep 29 17:11:30 2023\n",
      "1   bLog01D3.bin      4  Fri Sep 29 17:11:33 2023\n",
      "2   bLog01DA.bin      1  Fri Sep 29 17:11:37 2023\n",
      "3   bLog01DB.bin      3  Fri Sep 29 17:11:36 2023\n",
      "4   bLog01E0.bin      2  Fri Sep 29 17:11:45 2023\n",
      "..           ...    ...                       ...\n",
      "60  bLog02C1.bin      6  Fri Sep 29 17:11:55 2023\n",
      "61  bLog02C2.bin      1  Fri Sep 29 17:11:55 2023\n",
      "62  bLog02C3.bin      1  Fri Sep 29 17:11:55 2023\n",
      "63  bLog02E3.bin      2  Fri Sep 29 17:12:07 2023\n",
      "64  bLog0305.bin      2  Fri Sep 29 17:13:48 2023\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/elise/Desktop/Test data - BD3 3.16.23 -Drops Only'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\elise\\Documents\\GitHub\\BD-Code-Development\\BD code automation - phase 0.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elise/Documents/GitHub/BD-Code-Development/BD%20code%20automation%20-%20phase%200.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m stored_data[\u001b[39m\"\u001b[39m\u001b[39mRecording Time\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m record_time_list\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elise/Documents/GitHub/BD-Code-Development/BD%20code%20automation%20-%20phase%200.ipynb#X32sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(stored_data)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/elise/Documents/GitHub/BD-Code-Development/BD%20code%20automation%20-%20phase%200.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m stored_data\u001b[39m.\u001b[39;49mto_csv(outputPath, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, index\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3552\u001b[0m     path_or_buf,\n\u001b[0;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3568\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/elise/Desktop/Test data - BD3 3.16.23 -Drops Only'"
     ]
    }
   ],
   "source": [
    "stored_data = pd.DataFrame(columns=[\"File\", 'Drops', 'Recording Time'])\n",
    "\n",
    "file_name_list = []\n",
    "n_drops_list = []\n",
    "record_time_list = []\n",
    "\n",
    "file_list = os.listdir(binFolderPath)\n",
    "n = 0\n",
    "\n",
    "for filename in file_list:\n",
    "    if filename.endswith('.bin'):\n",
    "        n = n+1\n",
    "        print(\"on file\", n, \"of\", len(file_list), \":\", filename)\n",
    "        clear_output(wait = True)\n",
    "        readfile(filename)\n",
    "        accPick()\n",
    "        locate_drops()\n",
    "        if nDrops_trimmed > 0:\n",
    "            file_name_list.append(filename)\n",
    "            n_drops_list.append(nDrops_trimmed)\n",
    "            path  =  os.path.join(binFolderPath, filename)\n",
    "            c_time  = os.path.getctime(path)\n",
    "            local_time = time.ctime(c_time)\n",
    "            record_time_list.append(local_time)\n",
    "            peakplot(x, peaks)\n",
    "\n",
    "stored_data[\"File\"] = file_name_list\n",
    "stored_data[\"Drops\"] = n_drops_list\n",
    "stored_data[\"Recording Time\"] = record_time_list\n",
    "\n",
    "print(stored_data)\n",
    "stored_data.to_csv(outputPath, header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_data.to_csv(outputPath+\"/stored_data.csv\", header=None, index=None, sep=' ', mode='a')\n",
    "print(\"Number of Drops Found:\", sum(stored_data[\"\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
